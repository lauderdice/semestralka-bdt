//kazdodenni nahrani dat do Hive

hdfs dfs -put semestralka/prodeje/daily/* /user/lauderdice/semestralka/prodeje/daily
mv semestralka/prodeje/daily/* semestralka/prodeje/history

beeline -u "jdbc:hive2://hador-c1.ics.muni.cz:10000/default;principal=hive/hador-c1.ics.muni.cz@ICS.MUNI.CZ"
use semestralka_lauderdice;

DROP TABLE daily_sales_temp;
CREATE EXTERNAL TABLE daily_sales_temp ( datum string, date_block_num int, shop_id int, item_id int, item_price double,item_cnt_day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'  STORED AS TEXTFILE   LOCATION '/user/lauderdice/semestralka/prodeje/daily'  tblproperties("skip.header.line.count" = "1");

DROP TABLE daily_sales;
CREATE TABLE daily_sales ( datum string, shop_id int, item_id int, item_price double,item_cnt_day int, date_block_num int) stored as parquet tblproperties("parquet.compress"="SNAPPY");

INSERT overwrite table daily_sales select datum , shop_id, item_id, item_price, item_cnt_day, date_block_num from daily_sales_temp;
SELECT * FROM daily_sales limit 10;
DROP TABLE daily_sales_temp;


INSERT overwrite table sales partition (date_block_num) select datum , shop_id, item_id, item_price, item_cnt_day, date_block_num from sales UNION select datum , shop_id, item_id, item_price, item_cnt_day, date_block_num from daily_sales;

!exit
hdfs dfs -mv /user/lauderdice/semestralka/prodeje/daily/* /user/lauderdice/semestralka/prodeje/history

//tvorba reportu v PySparku

export PYSPARK_PYTHON=python3
module add python36-modules-gcc
export PYTHONIOENCODING=utf8
pyspark --master yarn --num-executors 2 --executor-memory 4G --conf spark.ui.port=11710
